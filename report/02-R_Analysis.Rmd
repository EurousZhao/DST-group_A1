---
title: "02-R_Analysis"
output: html_document
date: "2023-10-09"
---

# R Analysis

We advise you run the whole document (all chunks) now so that time consuming iterative processes at the end of the analysis are completed upon reading.

## Pre-requisites

Required Libraries:
```{r}
if(!require("ggplot2"))install.packages("ggplot2")
if(!require("corrplot"))install.packages("corrplot")
if(!require("caret"))install.packages("caret")
if(!require("rpart"))install.packages("rpart")
if(!require("reshape2"))install.packages("reshape2")
if(!require("dplyr"))install.packages("dplyr")
if(!require("rpart.plot"))install.packages("rpart.plot")
if(!require("caTools"))install.packages("caTools")
if(!require("xgboost")) install.packages("xgboost")
if(!require("pROC")) install.packages("pROC")
library(pROC)
library(xgboost)
library(ggplot2)
library(corrplot)
library(caret)
library(rpart)
library(reshape2)  # The reshape2 package is used for data reshaping.
library(dplyr) # The dplyr package is used for data manipulation.
library(rpart.plot)
library(caTools)
```

## Data

Call the data in from the directory created in '01_Data' and convert character columns into numeric binary valued columns so they can be correlated and analysed further.
```{r}
data <- read.table(path_wd("..","data","raw","smoking_driking_dataset.csv"),header=TRUE, sep = ",")
data <- as.data.frame(data)
df <- as.data.frame(data)
data$DRK_YN <- ifelse(data$DRK_YN == "Y", 1, 0)       # MUST ONLY BE RUN ONCE !
data$sex <- ifelse(data$sex == "Male", 1, 0)
# Convert DRK_YN and sex columns to numeric encoding
data$DRK_YN <- as.numeric(factor(data$DRK_YN))
data$sex <- as.numeric(factor(data$sex))
df$DRK_YN <- as.numeric(factor(df$DRK_YN))
df$sex <- as.numeric(factor(df$sex))
```


Some basic functions describing the data set.
```{r}
typeof(data)    # Self explanatory descriptive functions
class(data)
dim(data)
```

Below is an explanation of the headers:

Sex	- male, female	
age	- round up to 5 years	
height	- round up to 5 cm[cm]	
weight	- [kg]	
sight_left	- eyesight(left)	
sight_right	- eyesight(right)	
hear_left	- hearing left, 1(normal), 2(abnormal)	
hear_right	- hearing right, 1(normal), 2(abnormal)	
SBP	- Systolic blood pressure[mmHg]	 
DBP	- Diastolic blood pressure[mmHg]	 
BLDS	- BLDS or FSG(fasting blood glucose)[mg/dL]	
tot_chole	- total cholesterol[mg/dL]	 
HDL_chole	- HDL cholesterol[mg/dL]	HDL 
LDL_chole	- LDL cholesterol[mg/dL]	LDL 
triglyceride	- triglyceride[mg/dL]	
hemoglobin	- hemoglobin[g/dL]	
urine_protein	- protein in urine, 1(-), 2(+/-), 3(+1), 4(+2), 5(+3), 6(+4)	 
serum_creatinine	- serum(blood) creatinine[mg/dL] 
SGOT_AST	- SGOT(Glutamate-oxaloacetate transaminase) AST(Aspartate transaminase)[IU/L]	 AST
SGOT_ALT	- ALT(Alanine transaminase)[IU/L]	 ALT
gamma_GTP	- y-glutamyl transpeptidase[IU/L]	
SMK_stat_type_cd	- Smoking state, 1(never), 2(used to smoke but quit), 3(still smoke)	
DRK_YN	- Drinker or Not

Here are some descriptive functions of the variables.
```{r}
head(data)
summary(data)
str(data)     # Identify the types of each feature and the target variable
```

We have a data frame with 240,000 rows and 24 body data columns. There is a mixture of categorical, discrete and continuous variables. Some normally continuous features, such as height and weight, are discrete by rounding up to the nearest multiple of 5. This is worth noting when considering scatter plots and how they may have linear streaks of data on each discrete value although, this shouldn't affect the overall correlations much.

It is common practice to trim outlier data using a standard method such as the IQR method. We write a function that takes as input the data and vector of feature names and, using the IQR method, removes outliers in the named columns, outputting the refined data.
```{r}
remove_outliers_iqr <- function(data, columns) {
  for (column_name in columns) {
    Q1 <- quantile(data[[column_name]], 0.25)   # Quantile function
    Q3 <- quantile(data[[column_name]], 0.75)
    IQR <- Q3 - Q1
    lower_bound <- Q1 - 1.5 * IQR
    upper_bound <- Q3 + 1.5 * IQR
    
    data <- data[data[[column_name]] > lower_bound & data[[column_name]] < upper_bound, ]   # Jettison data falling outside of the bounds
  }
  
  return(data)
}
```

Having inspected our features using the *summary()* and *str()* functions, we know to apply outlier removal to non-categorical, continuous data.
```{r}
data_removed_outliers <- remove_outliers_iqr(data, c('waistline', 'sight_left', 'sight_right', 'SBP', 'DBP', 'BLDS', 'tot_chole', 'HDL_chole', 'LDL_chole', 'triglyceride', 'hemoglobin', 'serum_creatinine', 'SGOT_AST', 'SGOT_ALT', 'gamma_GTP'))
```

Contrasting to the previous summary, blatant anomalous data points such as waistline = 999.00 have been removed.
```{r}
summary(data_removed_outliers)
dim(data_removed_outliers)
```

Whilst there are evident outlier data points, the data origin states that all of the data is predictive in nature towards the target. So for matters of comparison and improved accuracy we proceed later (test and training data) with the original data. 

Check for empty columns.
```{r}
colSums(is.na(data))
```

No empty columns - we're good to go.

### Test/Train split

To train our models, we split the data into 20% test and 80% training. We also further split the data by having the 23 (predictor) features as X_train and X_test, and the target variable 'DRK_YN' as Y_train and Y_test.
```{r}
predictors <- data[, -which(names(data) == "DRK_YN")]
target <- data$DRK_YN

# Split the data into training and testing sets
set.seed(0)  # Set a random seed for reproducibility
splitIndex <- createDataPartition(target, p = 0.8, list = FALSE)
X_train <- predictors[splitIndex, ]
X_test <- predictors[-splitIndex, ]
Y_train <- target[splitIndex]
Y_test <- target[-splitIndex]
```


## EDA and plots

To perform some EDA we first create a random sample of size 10,000 so we plots don't take too long.
```{r}
# Sample 10,000 rows
sample_data <- data_removed_outliers[sample(nrow(data), 10000), ]
sample_data <- na.omit(sample_data)   # Omit empty rows for plotting
target_sample_data <- sample_data$DRK_YN

# 'sample_data' now contains 10,000 randomly selected rows from 'data'
head(sample_data)
```

Box plots for each column.
```{r}
# Adjusting plot margins and generating box plots for each column
par(mfrow = c(ceiling(ncol(your_data) / 100), 2))  # Setting up the layout for plots
par(mar = c(4, 4, 2, 1))  # Setting margin parameters: bottom, left, top, right

for (col in names(sample_data)) {
  boxplot(sample_data[[col]], main = col)  # Generating box plot for each column
}
```

A simple scatter to capture an aneccdotally known correlation.
```{r}
plot(sample_data$weight, sample_data$height, 
     xlab = "weight", ylab = "height", 
     main = "Scatter Plot of weight vs. height")    # Labels and title
```

Whilst not the most negative correlation, an interesting plot nonetheless. The relationship between high-density lipoprotein cholesterol and triglyceride levels is well-studied in the field of medicine due to it's impact on one's cardiovascular risk.
```{r}
plot(sample_data$HDL_chole,sample_data$triglyceride,
     xlab = "HDL Cholesterol", ylab = "Triglyceride Level", 
     main = "Scatter Plot of HDL Cholesterol vs. Triglyceride Level")
```

Here we plot two smoothed density curves for the hemoglobin feature, splitting into non-drinkers (salmon = 1) and drinkers (cyan = 2).
```{r}
target_sample_data <- as.factor(target_sample_data)   # Setting the target as a factor so the plot doesn't think the scale is from 1 to 2
ggplot(sample_data, aes(x = `hemoglobin`, group = target_sample_data, fill = target_sample_data)) +
  geom_density(alpha = 0.5)
```

Whilst both curves have a large area of cross-over there is enough of a discernible difference between the density functions for hemoglobin to have a significant correlation to the `DRK_YN`.

## Correlation and Feature Selection

We want to inspect how all the features correlate to see if we can reduce the feature selection and simplify the model for greater accuracy.
```{r}
correlation_matrix <- cor(data)

# Get the absolute correlation values of "target" with other variables
target_correlations <- abs(correlation_matrix[,"DRK_YN"])

# Sort the correlations in descending order
sorted_correlations <- sort(target_correlations, decreasing = TRUE)

# Print the sorted correlations
print(sorted_correlations)
```

If we count the variables that have the highest impact on drinking, we find that whether drinking has a large positive correlation with the four variables of sex, height, smoking, and hemoglobin concentration, and a large negative correlation with age. Most interestingly we see a person's height having the greatest correlation with whether they drink or not.

One possible explanation is that there is a large positive correlation between body size and gender, which is caused by the fact that men are generally taller than women. All we have to do is look at the relationship between gender and height, hemoglobin and smoking

Plotting the correlation matrix.
```{r}
corrplot(correlation_matrix, method = "color")
```

Visualizing the correlation matrix plot allows us to see this very quickly. No doubt, height, weight, hemoglobin and smoking have a very large positive correlation with gender!

For one method of covariate selection: we could obtain a subset of variables with greatest absolute correlation with `DRK_YN` which are all still highly uncorrelated with each other. However, we elect to use PCA in this instance.

### PCA

We apply PCA to the data and show the output princciapal components.
```{r}
# Scale the data for PCA
scaled_data <- scale(predictors)

# Apply PCA
pca_result <- prcomp(scaled_data, scale. = TRUE)

# Explained variance by each principal component
summary(pca_result)

# Retrieve transformed data with reduced dimensions
reduced_data <- as.data.frame(predict(pca_result))
```

We take the first 12 principal components which capture 85%> of the variance, a threshold we have set.

We reduce the PCA data down to the initial 12 PCs.
```{r}
data_pca <- as.data.frame(pca_result$x[, 1:12])
head(data_pca)
```

Plot the correlation matrix.
```{r}
correlation_matrix2 <- cor(data_pca)
corrplot(correlation_matrix2, method = "color")
```

We now create training and test data based on PC1-PC12 to see how it compares ahead with model accuracy

```{r}
predictors_pca <- data_pca[, -which(names(data) == "DRK_YN")]   # Same procedure as previous
target_pca <- data$DRK_YN

# Split the data into training and testing sets
set.seed(0)  # Set a random seed for reproducibility
splitIndex <- createDataPartition(target, p = 0.8, list = FALSE)
X_train_pca <- predictors_pca[splitIndex, ]
X_test_pca <- predictors_pca[-splitIndex, ]
Y_train_pca <- target_pca[splitIndex]
Y_test_pca <- target_pca[-splitIndex]
```

## Decision Trees

A decision tree is a fundamental machine learning model used for both classification and regression tasks. It is a hierarchical structure that makes decisions based on the values of input features. In a decision tree, the data is split into subsets based on the feature values, and at each internal node of the tree, a decision is made to determine the path to follow. The leaves of the tree represent the final output or prediction.

Run both training data sets on the decision tree model.
```{r}
# Assuming X_train is the matrix/data.frame of features and Y_train is the target variable
tree <- rpart(Y_train ~ ., data = X_train, method = "class")
tree_pca <- rpart(Y_train_pca ~ ., data = cbind(X_train_pca, Y_train_pca), method = "class")
```

Make prediction set against left-out test data for both the normal and PCA-transformed data sets, and show Confusion Matrix, detailing various performance metrics, for both.
```{r}
test_pred <- predict(tree, X_test, type = "class")
test_pred_pca <- predict(tree_pca, X_test_pca, type = "class")

test_confMat <- confusionMatrix(test_pred, as.factor(Y_test))
test_confMat_pca <- confusionMatrix(test_pred_pca, as.factor(Y_test_pca))

print(test_confMat)
print(test_confMat_pca)
```

Results are discussed at the end of this section.

### Optimisation

The decision is tree is optimised to find the 'best' complexity level, to see if we can yield greater accuracy in the Confusion Matrix.

Warning: This takes ~1min to run - it can be skipped as the result is explained below.
```{r}
# Set up cross-validation
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

# Create the model using rpart and tune parameters
model <- train(Y_train ~ ., data = cbind(X_train, Y_train), method = "rpart",
               trControl = control,
               tuneGrid = expand.grid(cp = seq(0.01, 0.5, by = 0.01)))  # Vary cp parameter

# Show the best parameters found
print(model)

# To plot the decision tree
final_tree <- rpart(Y_train ~ ., data = cbind(X_train, Y_train), method = "class", cp = model$bestTune$cp)
plot(final_tree)
text(final_tree)

```

The optimal complexity is found to be 0.01 which yields the same results as the original tree. So in the scope of decision tree factors, we have the highest performance metric levels.

### Visualisation of Results

Visualise the Confusion Matrices.
```{r}
test_confMatdf <- as.data.frame(test_confMat$table)
ggplot(data = test_confMatdf, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 1) +
  labs(title = "Confusion Matrix", x = "Actual", y = "Predicted")

test_confMatdf_pca <- as.data.frame(test_confMat_pca$table)
ggplot(data = test_confMatdf_pca, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 1) +
  labs(title = "Confusion Matrix", x = "Actual", y = "Predicted")
```

Plot the trees.
```{r}
rpart.plot(tree)
rpart.plot(tree_pca)
```
We see an even greater simplicity in the tree ran on PCA-transformed data, indicating the principal components captured too low a level of variance.

### Decision Tree Analysis

The best recall/sensitivity we could obtain from the decision tree is 74.5% (2d.p.) using all the features, whilst we had 69% (2d.p.) on the PCA-transformed dataset.

Whilst not as high as we'd like, we can be happy that the Sensitivity is higher than Specificity, which is 65% (2d.p.) using all features, as we've outlined in our intro to be the most important performance metric.

When trying to maximize accuracy against depth and complexity we found the lowest level of complexity to achieve greatest results. Given that the correlations of all the features with the target are low we can rule out model simplicity where a decision tree may fall short. We can also reasonably rule out the homogeneity of the features as using Principal Component Analysis with 12 entirely uncorrelated Principal Components that still captured a high enough threshold of the data, we still saw lower accuracy. 

We maintain that the model is good given its 74% true positive rate.


## XGBoost

XGBoost, short for eXtreme Gradient Boosting, It falls under the category of ensemble learning methods and is based on decision tree models. XGBoost trains models in a gradient-boosting manner.
 It falls under the category of ensemble learning methods and is based on decision tree models. XGBoost trains models in a gradient-boosting manner. 
code based on (https://www.kaggle.com/code/raman209/prediction-of-drinkers-using-body-signals)

We first convert the drinking data into numbers to facilitate our binary classification

```{r}
# Convert DRK_YN and sex columns to numeric encoding
df$DRK_YN <- as.numeric(factor(df$DRK_YN))
df$sex <- as.numeric(factor(df$sex))

# Replace 1 with 0 and 2 with 1 in the DRK_YN column
df$DRK_YN[df$DRK_YN == 1] <- 0
df$DRK_YN[df$DRK_YN == 2] <- 1
```

```{r}
# Select the feature columns
X <- df[, !names(df) %in% c("DRK_YN")]
# Select the target variable column
y <- df$DRK_YN
```

Since we mentioned before that drinking is related to age, let’s try to visualize the age distribution

```{r}
# Create a ggplot chart
p <- ggplot(data = df, aes(x = age)) +
  geom_histogram(binwidth = 5, fill = "lightblue", color = "black") +
  geom_density(color = "red") +
  labs(x = "Age", y = "Frequency", title = "Distribution of Age")

# Display the chart
print(p)
```
The age data are mainly concentrated in the 35-65 age group, which shows that the subjects who provided the data are generally middle-aged and elderly people.

We split the data into training data and test data according to the ratio of 80%

```{r}
# Set a random seed to ensure reproducible results
set.seed(123)

test_size <- 0.2  # 20% of the data is used for testing

# Split the data using the sample.split function
split <- sample.split(y, SplitRatio = 1 - test_size)

# Create the training set
X_train <- X[split, ]
y_train <- y[split]

# Create the test set
X_test <- X[!split, ]
y_test <- y[!split]
```

Now we use xgboost for data fitting, and the number of boost rounds is set to 25.This number was obtained later through subsequent attempts. Learning rate set to 0.3

```{r}
# Install the xgboost package if not already installed
if (!require("xgboost")) install.packages("xgboost")
library(xgboost)


# Create an XGBoost classifier model round = 100
model <- xgboost(data = as.matrix(X_train), label = y_train,eta <- 0.3, objective = "binary:logistic", nrounds = 25)

# Use the model for predictions
y_pred <- predict(model, as.matrix(X_test))
# Print the prediction results
print(y_pred)
```

Next we test the accuracy of the model using a confusion matrix.

```{r}
# Custom rounding function
custom_round <- function(x) {
  ifelse(x < 0.5, 0, 1)
}

# Convert y_pred to binary classification labels
y_pred_binary <- ifelse(y_pred >= 0.5, 1, 0)

# Convert y_test to a factor
y_test <- factor(y_test, levels = c(0, 1))

# Convert y_pred_binary to a factor
y_pred_binary <- factor(y_pred_binary, levels = c(0, 1))

# Ensure they have the correct levels
levels_y_test <- levels(factor(y_test))
y_pred_binary <- factor(y_pred_binary, levels = levels_y_test)

# Now you can calculate accuracy and generate a classification report
accuracy <- confusionMatrix(data = y_pred_binary, reference = y_test)$overall["Accuracy"]
cat("Accuracy: ", format(accuracy, nsmall = 2), "\n")

# Generate a classification report
confusion_matrix <- confusionMatrix(data = y_pred_binary, reference = y_test)
print(confusion_matrix)
```

The True Positive Rate is very important for model compare, And the recall rate of xgboost is:
```{r}
# Calculate True Positives (TP) and False Negatives (FN)
TP <- sum(y_test == 1 & y_pred_binary == 1)
FN <- sum(y_test == 1 & y_pred_binary == 0)

# Calculate Recall
recall <- TP / (TP + FN)

cat("Recall (True Positive Rate): ", format(recall, nsmall = 2), "\n")

```

## The relationship between accuracy and number of iterations

In the normal settings of xgboost, the accuracy will change slightly with the different number of rounds, and its value is not monotonically increasing. It will reach the maximum value in a certain interval.

Warning: Takes ~2min to run.
```{r}
# Data preparation
X_train <- as.matrix(X_train)
X_test <- as.matrix(X_test)

# Treat labels as numeric if they are for binary classification
y_train <- as.numeric(y_train)

# List of different iteration rounds
rounds <- c(1,5,10,25, 50, 75, 100, 125, 150, 200)  # You can add more rounds as needed

# Store models and accuracies for different iteration rounds
models <- list()
accuracies <- numeric(length(rounds))

# Loop to train models for different iteration rounds
for (i in 1:length(rounds)) {
  # Create an XGBoost classification model
  model <- xgboost(data = X_train, label = y_train, objective = "binary:logistic", nrounds = rounds[i])
  models[[i]] <- model  # Store the model
  
  # Make predictions using the model
  y_pred <- predict(model, X_test)
  
  # Set the threshold
  threshold <- 0.5
  
  # Convert probability values to binary classification labels
  y_pred_binary <- ifelse(y_pred >= threshold, 1, 0)
  
  # Calculate accuracy
  accuracy <- mean(y_pred_binary == y_test)
  accuracies[i] <- accuracy  # Store accuracy
}

# Output accuracies for different iteration rounds
results <- data.frame(Rounds = rounds, Accuracy = accuracies)
print(results)
```

We want to explore the relationship between accuracy and number of iterations

```{r}
# Create a data frame to store the results
results_df <- data.frame(Rounds = rounds, Accuracy = accuracies)

# Fit a polynomial regression model (using a 6th-degree polynomial, you can adjust the degree as needed)
poly_fit <- lm(Accuracy ~ poly(Rounds, 4), data = results_df)

# Create a new data frame for plotting the fitted curve
plot_data <- data.frame(Rounds = seq(min(rounds), max(rounds), length.out = 100))

# Predict the values of the fitted curve
plot_data$Accuracy <- predict(poly_fit, newdata = data.frame(Rounds = plot_data$Rounds))

# Find the maximum accuracy point on the fitted curve
max_accuracy <- max(plot_data$Accuracy)
max_round <- plot_data$Rounds[which.max(plot_data$Accuracy)]
```


```{r}
# Create the plot object, set the axis limits
#p <- ggplot(results_df, aes(x = Rounds, y = Accuracy)) +
#  geom_point() +  # Plot the original data points
  #geom_line(data = plot_data, aes(x = Rounds, y = Accuracy), color = "blue") +  # Plot the fitted curve
#  labs(x = "Rounds", y = "Accuracy") +  # Set axis labels
#  theme_minimal() +  # Set the plot theme
#  ylim(0.72, 0.745)  # Set the y-axis limits

# Add a label for the maximum value
#p <- p + annotate("text", x = max_round, y = max_accuracy, label = paste("Max Accuracy =", round(max_accuracy, 4)), vjust = -1, hjust = 0.5, color = "red")

# Add a label for the maximum value's corresponding round
#p <- p + annotate("text", x = max_round, y = max_accuracy, label = paste("at Rounds =", round(max_round, 0)), vjust = 1, hjust = 0.5, color = "red")

# Display the plot
#print(p)
```

Draw the curve of Rounds and Accuracy
```{r}
# Create the plot object, set the axis limits
p <- ggplot(results_df, aes(x = Rounds, y = Accuracy)) +
  geom_point() +  # Plot the original data points
  geom_smooth(method = "loess", color = "blue") +  # Add a smoothed curve
  labs(x = "Rounds", y = "Accuracy") +  # Set axis labels
  theme_minimal() +  # Set the plot theme
  ylim(0.72, 0.745)  # Set the y-axis limits

# Add a label for the maximum value
p <- p + annotate("text", x = max_round, y = max_accuracy, label = paste("Max Accuracy =", round(max_accuracy, 4)), vjust = -1, hjust = 0.5, color = "red")

# Add a label for the maximum value's corresponding round
p <- p + annotate("text", x = max_round, y = max_accuracy, label = paste("at Rounds =", round(max_round, 0)), vjust = 1, hjust = 0.5, color = "red")

# Display the plot
print(p)

```

The maximum accuracy is in the rounds interval 25-100, which is approximately 0.738.

A lot of information can be obtained from this curve. First, The rounds parameter in XGBoost determines the complexity of the model. A lower value of rounds can lead to underfitting, where the model is too simple to capture the underlying patterns in the data. Conversely, a higher value of rounds can result in overfitting, where the model becomes excessively complex and starts fitting the noise in the training data. In this model, the rounds value is set to 25 to ensure that the max recall rate.

On the other hand, the accuracy of the model has stabilized after more than 30 iterations. The xgboost model has shown high accuracy with a very small number of iterations, and it can be considered that its convergence of accuracyis very fast.

##Analyze three models using ROC curves

```{r}
roc_curve = roc(Y_test, as.numeric(test_pred))
roc_curve2 = roc(Y_test_pca, as.numeric(test_pred_pca))
roc_curve3 = roc(y_pred_binary, as.numeric(y_test))
plot(roc_curve, col = "blue", main = "ROC Curve")
lines(roc_curve2, col = "red")
lines(roc_curve3, col = "black")
legend("bottomright", legend = c("Decision Tree", "PCA-transfromed Decision Tree", "XGboost"), col = c("blue", "red","black"), lty = 1)
```

Xgboost model has a higher Area Under the Receiver Operating Characteristic Curve (AUC) compared to Decision Tree, it indicates that Xgboost has better discrimination or predictive ability in distinguishing between the drinker and non-drinker in this binary classification problem.

## Analysis of xgboost

The True Positive Rate provided by xgboost is about 0.738, which is almost equal to the max accuracy it provides.
Compared with the decision tree model, xgboost did not significantly improve the accuracy of prediction, which may be due to many reasons. One of the possible reasons is the data itself.Due to its characteristics, ours models in most cases may not provide higher accuracy unless more detailed or targeted processing is performed.

In terms of boosting, xgboost algorithm combines multiple weak learners (such as decision trees) to create a strong learner. So called 'Gradient Boosting'.It is an important part of the xgboost algorithm.The Gradient Boosting process works as follows:

A base model (usually a simple decision tree) is trained on the dataset.

The base model's predictions are used to compute the residuals, which are the differences between the true target values and the model's predictions.

A new base model is then trained to predict these residuals. This new model focuses on the errors made by the previous model.

The predictions of the new model are combined with the predictions of the previous models, and the residuals are updated.

This process is repeated iteratively, with each new model learning to correct the errors made by the existing ensemble. Controlled according to the number of iteration rounds, reaching the maximum value within a certain range.

Another powerful thing about the xgboost model is its saving of computing time! After the number of rounds reaches more than 10 times, the change in accuracy has been less than 1%.
In actual use, 15-25 boost rounds can meet the demand. Even when processing larger data, the xgboost algorithm will greatly reduce the calculation time. In other words, the xgboost algorithm is excellent in both efficiency and accuracy.

One of the way xgboost can be accomplished the requirement with a smaller number of iterations rounds is it limits the size of the trees. XgBoost incorporates L1 (Lasso) and L2 (Ridge) regularization terms into its objective function.For example,
adds a penalty term to the objective function that encourages the model to have sparse feature importance. It does this by adding the absolute values of the weights of the features to the objective function. As a result, it can drive some feature weights to exactly zero, 
effectively selecting a subset of the most important features. These regularization terms help control the complexity of individual trees and the ensemble.
By constraining the weights of the trees, xgBoost prevents them from becoming too deep or overfitting the training data. Regularization can lead to faster convergence by reducing the tendency of the model to overfit.

Setting the learning rate can also improve efficiency. The learning rate determines how far the model parameters will advance in the direction of the negative gradient with each parameter update. 
A smaller learning rate results in a small step size, making parameter updates more cautious and contributing to more stable convergence, but may require more iterations to reach the optimal solution.A smaller learning rate can make the optimization process more stable but slower,
while a larger learning rate can speed up convergence but may result in overshooting the minimum. Here we use the default learning rate of 0.3.
If we reduce it to 0.1, the accuracy will be slightly improved, but the calculation speed also be slower.

Overall, XGBoost excels in efficiency and accuracy but demands expertise in configuration and understanding of data to fully leverage its capabilities.
It can effectively analyze complex data accurately and quickly, and the effect is usually stronger than most models.
If want to obtain a model that better meets your needs, you must manually try many parameters (such as the number of iterations, learning rate) many times to obtain it. 

In terms of results, this model performs well in solving our problem and meets our needs.

## Analysis of two classification methods

For this particular dataset, both Decision Trees and XGBoost achieved similar accuracy levels, but XGBoost demonstrated superior speed. Additionally, the decrease in accuracy for Decision Trees after using PCA suggests that feature engineering and preprocessing are critical considerations when working with Decision Trees. However, XGBoost seems more resilient to dimensionality reduction, which is an advantage in scenarios where feature engineering is required.

Specifically, we compare the following points:

Accuracy: Both Decision Trees and XGBoost exhibit similar accuracy levels, both around 74%. This suggests that, for this specific dataset, Decision Trees, despite their simplicity, can perform on par with the more complex XGBoost algorithm.

Speed: XGBoost outperforms Decision Trees in terms of speed. It is a gradient boosting algorithm that is optimized for efficiency, making it a faster option for training and making predictions. This can be especially advantageous for large datasets or real-time applications.

Preprocessing Impact: It's interesting to note that when using PCA for preprocessing, the accuracy of the Decision Tree model decreased. This indicates that the feature reduction caused by PCA may have removed some information important for Decision Trees, leading to a lower accuracy. On the other hand, XGBoost may be more robust to this reduction in dimensionality due to its ensemble nature and feature selection capabilities.


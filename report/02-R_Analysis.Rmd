---
title: "02-R_Analysis"
output: html_document
date: "2023-10-09"
---

# R Analysis

## Pre-requisites

Required Libraries:
```{r}
if(!require("ggplot2"))install.packages("ggplot2")
if(!require("corrplot"))install.packages("corrplot")
if(!require("caret"))install.packages("caret")
if(!require("rpart"))install.packages("rpart")
if(!require("reshape2"))install.packages("reshape2")
if(!require("dplyr"))install.packages("dplyr")
if(!require("rpart.plot"))install.packages("rpart.plot")
if(!require("caTools"))install.packages("caTools")
if(!require("xgboost")) install.packages("xgboost")
if(!require("pROC")) install.packages("pROC")
library(pROC)
library(xgboost)
library(ggplot2)
library(corrplot)
library(caret)
library(rpart)
library(reshape2)  # The reshape2 package is used for data reshaping.
library(dplyr) # The dplyr package is used for data manipulation.
library(rpart.plot)
library(caTools)
```

## Data

```{r}
data <- read.table("C:/Users/User/Desktop/smoking_driking_dataset.csv",header=TRUE, sep = ",")
dataog <- read.table("C:/Users/User/Desktop/smoking_driking_dataset.csv",header=TRUE, sep = ",")
data <- as.data.frame(data)
df <- as.data.frame(data)
data$DRK_YN <- ifelse(data$DRK_YN == "Y", 1, 0)       # MUST ONLY BE RUN ONCE !
data$sex <- ifelse(data$sex == "Male", 1, 0)
# Convert DRK_YN and sex columns to numeric encoding
dataog$DRK_YN <- as.numeric(factor(df$DRK_YN))
dataog$sex <- as.numeric(factor(df$sex))
```


```{r}
typeof(data)
class(data)
dim(data)
head(data)
summary(data)
str(data)
```

##Remove outliers e.g. waistline 999

```{r}
# Function to remove outliers using the IQR method for all numerical columns
#remove_outliers_all_columns <- function(df) {
 # numerical_cols <- sapply(df, is.numeric)  # Find numeric columns
  
  # Loop through numerical columns
  #for (col in names(df)[numerical_cols]) {
    #column <- df[[col]]
   # 
    # Calculate the IQR
  #  Q1 <- quantile(column, 0.25)
   # Q3 <- quantile(column, 0.75)
    #IQR <- Q3 - Q1
    
    # Define the lower and upper bounds
    #lower_bound <- Q1 - 1.5 * IQR
  #  upper_bound <- Q3 + 1.5 * IQR
    
    # Filter the dataframe to remove outliers for each column
   # df <- df[column >= lower_bound & column <= upper_bound, ]
  #}
  
#  return(df)
#}

# Remove outliers in all numerical columns of the dataframe 'df'
#data <- remove_outliers_all_columns(data)

```

```{r}
summary(data)
```
```{r}
summary(dataog)
```


Below is an explanation of these variables, which represent body data:

Sex	male, female	
age	round up to 5 years	
height	round up to 5 cm[cm]	
weight	[kg]	
sight_left	eyesight(left)	
sight_right	eyesight(right)	
hear_left	hearing left, 1(normal), 2(abnormal)	
hear_right	hearing right, 1(normal), 2(abnormal)	
SBP	Systolic blood pressure[mmHg]	 
DBP	Diastolic blood pressure[mmHg]	 
BLDS	BLDS or FSG(fasting blood glucose)[mg/dL]	
tot_chole	total cholesterol[mg/dL]	 
HDL_chole	HDL cholesterol[mg/dL]	HDL 
LDL_chole	LDL cholesterol[mg/dL]	LDL 
triglyceride	triglyceride[mg/dL]	
hemoglobin	hemoglobin[g/dL]	
urine_protein	protein in urine, 1(-), 2(+/-), 3(+1), 4(+2), 5(+3), 6(+4)	 
serum_creatinine	serum(blood) creatinine[mg/dL] 
SGOT_AST	SGOT(Glutamate-oxaloacetate transaminase) AST(Aspartate transaminase)[IU/L]	 AST
SGOT_ALT	ALT(Alanine transaminase)[IU/L]	 ALT
gamma_GTP	y-glutamyl transpeptidase[IU/L]	
SMK_stat_type_cd	Smoking state, 1(never), 2(used to smoke but quit), 3(still smoke)	
DRK_YN	Drinker or Not


We first convert the drinking data into numbers to facilitate our binary classification
```{r}
# Convert DRK_YN and sex columns to numeric encoding
df$DRK_YN <- as.numeric(factor(df$DRK_YN))
df$sex <- as.numeric(factor(df$sex))

data$DRK_YN <- as.numeric(factor(data$DRK_YN))
data$sex <- as.numeric(factor(data$sex))
```


```{r}
colSums(is.na(data))
```

### Test/Train split

```{r}
# Assuming 'dataset' is your data frame
predictors <- data[, -which(names(data) == "DRK_YN")]
target <- data$DRK_YN

# Split the data into training and testing sets
set.seed(0)  # Set a random seed for reproducibility
splitIndex <- createDataPartition(target, p = 0.8, list = FALSE)
X_train <- predictors[splitIndex, ]
X_test <- predictors[-splitIndex, ]
Y_train <- target[splitIndex]
Y_test <- target[-splitIndex]
```


## EDA and plots

```{r}
# Sample 10,000 rows from your_data
sada <- data[sample(nrow(data), 10000), ]
targetsada <- sada$DRK_YN

# 'sada' now contains 10,000 randomly selected rows from 'data'
head(sada)
```

```{r}
# Example dataset (replace this with your own dataframe)
your_data <- dataog  # Example dataset, you can replace this with your own dataframe

# Adjusting plot margins and generating box plots for each column
par(mfrow = c(ceiling(ncol(your_data) / 100), 2))  # Setting up the layout for plots
par(mar = c(4, 4, 2, 1))  # Setting margin parameters: bottom, left, top, right

for (col in names(your_data)) {
  boxplot(your_data[[col]], main = col)  # Generating box plot for each column
}

```


```{r}
# Create a scatter plot
plot(sada$weight, sada$height, 
     xlab = "weight", ylab = "height", 
     main = "Scatter Plot of x vs. y")
```

```{r}
plot(sada$hemoglobin,sada$gamma_GTP)
```

```{r}
ggplot(sada, aes(x = `hemoglobin`, group = targetsada, fill = targetsada)) +
  geom_density(alpha = 0.5)
```

##  Feature Selection and Correlation - Test/train split + Feature selection + correlation/PCA

```{r}
correlation_matrix <- cor(data)

# Get the absolute correlation values of "target" with other variables
target_correlations <- abs(correlation_matrix[,"DRK_YN"])

# Sort the correlations in descending order
sorted_correlations <- sort(target_correlations, decreasing = TRUE)

# Print the sorted correlations
print(sorted_correlations)
```

```{r}
corrplot(correlation_matrix, method = "color")
```


If we count the variables that have the highest impact on drinking, we find that whether drinking has a large positive correlation with the four variables of sex, height, smoking, and hemoglobin concentration, and a large negative correlation with age. 
Among them, height has the greatest impact, which may be unexpected.

One possible explanation is that there is a large positive correlation between body size and gender, which is caused by the fact that men are generally taller than women. All we have to do is look at the relationship between gender and height, hemoglobin and smoking

Visualizing the correlation matrix plot allows us to see this very quickly.No doubt, height, weight, hemoglobin and smoking have a very large positive correlation with gender!
This means that many of these variables work together.

### picking only uncorrelated features

```{r}
# Assuming 'correlation_matrix' is your correlation matrix
#high_correlation_threshold <- 0.85  # Define a correlation threshold

# Find highly correlated features
#highly_correlated <- findCorrelation(correlation_matrix, cutoff = high_correlation_threshold)

# Remove highly correlated features
#reduced_features <- X_train[, -highly_correlated]
#print(reduced_features)
```

### PCA

```{r}
# Scale the data for PCA (optional but recommended)
scaled_data <- scale(predictors)

# Apply PCA
pca_result <- prcomp(scaled_data, scale. = TRUE)

# Explained variance by each principal component
summary(pca_result)

# Retrieve transformed data with reduced dimensions
reduced_data <- as.data.frame(predict(pca_result))
```

We take the first 12 principal components which capture 80% of the variance

```{r}
data_pca <- as.data.frame(pca_result$x[, 1:12])
correlation_matrix2 <- cor(data_pca)
corrplot(correlation_matrix2, method = "color")
```

We'll reduce the PCA result data frame into our 12 PCs that capture our vairance tolerance

```{r}
head(data_pca)
```

We now create training and test data based on PC1-PC12 to see how it compares ahead with model accuracy

```{r}
# Assuming 'dataset' is your data frame
predictors_pca <- data_pca[, -which(names(data) == "DRK_YN")]
target_pca <- data$DRK_YN

# Split the data into training and testing sets
set.seed(0)  # Set a random seed for reproducibility
splitIndex <- createDataPartition(target, p = 0.8, list = FALSE)
X_train_pca <- predictors_pca[splitIndex, ]
X_test_pca <- predictors_pca[-splitIndex, ]
Y_train_pca <- target_pca[splitIndex]
Y_test_pca <- target_pca[-splitIndex]
```

## Decision Trees
A decision tree is a fundamental machine learning model used for both classification and regression tasks. It is a hierarchical structure that makes decisions based on the values of input features. In a decision tree, the data is split into subsets based on the feature values, and at each internal node of the tree, a decision is made to determine the path to follow. The leaves of the tree represent the final output or prediction.
```{r}
# Assuming X_train is the matrix/data.frame of features and Y_train is the target variable
tree <- rpart(Y_train ~ ., data = X_train, method = "class")
tree_pca <- rpart(Y_train_pca ~ ., data = cbind(X_train_pca, Y_train_pca), method = "class")
```

```{r}
test_pred <- predict(tree, X_test, type = "class")
test_pred_pca <- predict(tree_pca, X_test_pca, type = "class")

test_confMat <- confusionMatrix(test_pred, as.factor(Y_test))
test_confMat_pca <- confusionMatrix(test_pred_pca, as.factor(Y_test_pca))

print(test_confMat)
print(test_confMat_pca)
```

```{r}
predic <- predict(tree, X_test, type = "class")
confusionMatrix(predic, as.factor(Y_test))

```


```{r}
# Set up cross-validation
control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

# Create the model using rpart and tune parameters
model <- train(Y_train ~ ., data = cbind(X_train, Y_train), method = "rpart",
               trControl = control,
               tuneGrid = expand.grid(cp = seq(0.01, 0.5, by = 0.01)))  # Vary cp parameter

# Show the best parameters found
print(model)

# To plot the decision tree
final_tree <- rpart(Y_train ~ ., data = cbind(X_train, Y_train), method = "class", cp = model$bestTune$cp)
plot(final_tree)
text(final_tree)

```


```{r}
trex <- rpart(Y_train ~ ., data = X_train, method = "class", control = rpart.control(maxdepth = 10, cp = 0.01))
predx <- predict(trex, X_test, type = "class")
confusionMatrix(predx, as.factor(Y_test))
```

This part uses visual processing of confusion matrices
```{r}
test_confMatdf <- as.data.frame(test_confMat$table)
ggplot(data = test_confMatdf, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 1) +
  labs(title = "Confusion Matrix", x = "Actual", y = "Predicted")

test_confMatdf_pca <- as.data.frame(test_confMat_pca$table)
ggplot(data = test_confMatdf_pca, aes(x = Reference, y = Prediction, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = 1) +
  labs(title = "Confusion Matrix", x = "Actual", y = "Predicted")
```

```{r}
rpart.plot(tree)
rpart.plot(tree_pca)
```

```{r}
roc_curve = roc(Y_test, as.numeric(test_pred))
roc_curve2 = roc(Y_test_pca, as.numeric(test_pred_pca))
plot(roc_curve, col = "blue", main = "ROC Curve")
lines(roc_curve2, col = "red")
legend("bottomright", legend = c("Decision Tree", "PCA-transfromed Decision Tree"), col = c("blue", "red"), lty = 1)
```

### Decision Tree Analysis

The best recall/sensitivity we could obtain from the decision tree is 74% (2d.p.) using all the features, whilst we had 69% (2d.p.) on the PCA-transformed dataset.

Whilst not high we can be happy that the Sensitivity is higher than specificity, which is 65% (2d.p.) using all features, as we've outlined in our intro to be the most important performance metric.

When trying to maximize accuracy against depth and complexity we found the lowest level of complexity to achieve greatest results. Given that the correlations of all the features with the target are low we can rule out model simplicity where a decision tree may fall short. We can also reasonably rule out the homogeneity of the features as using Principal Component Analysis with 12 entirely uncorrelated Principal Components that still captured a high enough threshold of the data, we still saw lower accuracy. 

we maintain that the model is good given it's 74% true positive rate.


## XGBoost

XGBoost, short for eXtreme Gradient Boosting, It falls under the category of ensemble learning methods and is based on decision tree models. XGBoost trains models in a gradient-boosting manner.
 It falls under the category of ensemble learning methods and is based on decision tree models. XGBoost trains models in a gradient-boosting manner. 
code based on (https://www.kaggle.com/code/raman209/prediction-of-drinkers-using-body-signals)

We first convert the drinking data into numbers to facilitate our binary classification

```{r}
# Convert DRK_YN and sex columns to numeric encoding
df$DRK_YN <- as.numeric(factor(df$DRK_YN))
df$sex <- as.numeric(factor(df$sex))

# Replace 1 with 0 and 2 with 1 in the DRK_YN column
df$DRK_YN[df$DRK_YN == 1] <- 0
df$DRK_YN[df$DRK_YN == 2] <- 1
```

```{r}
# Select the feature columns
X <- df[, !names(df) %in% c("DRK_YN")]
# Select the target variable column
y <- df$DRK_YN
```

Since we mentioned before that drinking is related to age, letâ€™s try to visualize the age distribution

```{r}
# Create a ggplot chart
p <- ggplot(data = df, aes(x = age)) +
  geom_histogram(binwidth = 5, fill = "lightblue", color = "black") +
  geom_density(color = "red") +
  labs(x = "Age", y = "Frequency", title = "Distribution of Age")

# Display the chart
print(p)
```
The age data are mainly concentrated in the 35-65 age group, which shows that the subjects who provided the data are generally middle-aged and elderly people.

We split the data into training data and test data according to the ratio of 80%

```{r}
# Set a random seed to ensure reproducible results
set.seed(123)

test_size <- 0.2  # 20% of the data is used for testing

# Split the data using the sample.split function
split <- sample.split(y, SplitRatio = 1 - test_size)

# Create the training set
X_train <- X[split, ]
y_train <- y[split]

# Create the test set
X_test <- X[!split, ]
y_test <- y[!split]
```

Now we use xgboost for data fitting, and the number of boost rounds is set to 100.This number was obtained later through subsequent attempts.

```{r}
# Install the xgboost package if not already installed
if (!require("xgboost")) install.packages("xgboost")
library(xgboost)

# Create an XGBoost classifier model round = 100
model <- xgboost(data = as.matrix(X_train), label = y_train, objective = "binary:logistic", nrounds = 100)

# Use the model for predictions
y_pred <- predict(model, as.matrix(X_test))
# Print the prediction results
print(y_pred)
```

Next we test the accuracy of the model using a confusion matrix.

```{r}
# Custom rounding function
custom_round <- function(x) {
  ifelse(x < 0.5, 0, 1)
}

# Convert y_pred to binary classification labels
y_pred_binary <- ifelse(y_pred >= 0.5, 1, 0)

# Convert y_test to a factor
y_test <- factor(y_test, levels = c(0, 1))

# Convert y_pred_binary to a factor
y_pred_binary <- factor(y_pred_binary, levels = c(0, 1))

# Ensure they have the correct levels
levels_y_test <- levels(factor(y_test))
y_pred_binary <- factor(y_pred_binary, levels = levels_y_test)

# Now you can calculate accuracy and generate a classification report
accuracy <- confusionMatrix(data = y_pred_binary, reference = y_test)$overall["Accuracy"]
cat("Accuracy: ", format(accuracy, nsmall = 2), "\n")

# Generate a classification report
confusion_matrix <- confusionMatrix(data = y_pred_binary, reference = y_test)
print(confusion_matrix)
```

And the recall rate is:
```{r}
# Calculate True Positives (TP) and False Negatives (FN)
TP <- sum(y_test == 1 & y_pred_binary == 1)
FN <- sum(y_test == 1 & y_pred_binary == 0)

# Calculate Recall
recall <- TP / (TP + FN)

cat("Recall (True Positive Rate): ", format(recall, nsmall = 2), "\n")

```

xgboost did not significantly improve the accuracy of prediction, which may be due to many reasons.
One of the possible reasons is the data itself. It is very difficult to improve the accuracy of prediction when using all features. Our python code shows the accuracy of some model regressions, which almost all stop at around 75%. , making it almost impossible to obtain higher accuracy without more processing of the data.

In the normal settings of xgboost, the accuracy will change slightly with the different number of rounds, and its value is not monotonically increasing.
```{r}
# Data preparation
X_train <- as.matrix(X_train)
X_test <- as.matrix(X_test)

# Treat labels as numeric if they are for binary classification
y_train <- as.numeric(y_train)

# List of different iteration rounds
rounds <- c(1,5,10, 50, 75, 100, 125, 150, 200)  # You can add more rounds as needed

# Store models and accuracies for different iteration rounds
models <- list()
accuracies <- numeric(length(rounds))

# Loop to train models for different iteration rounds
for (i in 1:length(rounds)) {
  # Create an XGBoost classification model
  model <- xgboost(data = X_train, label = y_train, objective = "binary:logistic", nrounds = rounds[i])
  models[[i]] <- model  # Store the model
  
  # Make predictions using the model
  y_pred <- predict(model, X_test)
  
  # Set the threshold
  threshold <- 0.5
  
  # Convert probability values to binary classification labels
  y_pred_binary <- ifelse(y_pred >= threshold, 1, 0)
  
  # Calculate accuracy
  accuracy <- mean(y_pred_binary == y_test)
  accuracies[i] <- accuracy  # Store accuracy
}

# Output accuracies for different iteration rounds
results <- data.frame(Rounds = rounds, Accuracy = accuracies)
print(results)
```
```{r}
# Create a data frame to store the results
results_df <- data.frame(Rounds = rounds, Accuracy = accuracies)

# Fit a polynomial regression model (using a 6th-degree polynomial, you can adjust the degree as needed)
poly_fit <- lm(Accuracy ~ poly(Rounds, 7), data = results_df)

# Create a new data frame for plotting the fitted curve
plot_data <- data.frame(Rounds = seq(min(rounds), max(rounds), length.out = 100))

# Predict the values of the fitted curve
plot_data$Accuracy <- predict(poly_fit, newdata = data.frame(Rounds = plot_data$Rounds))

# Find the maximum accuracy point on the fitted curve
max_accuracy <- max(plot_data$Accuracy)
max_round <- plot_data$Rounds[which.max(plot_data$Accuracy)]

# Create the plot object, set the axis limits
p <- ggplot(results_df, aes(x = Rounds, y = Accuracy)) +
  geom_point() +  # Plot the original data points
  #geom_line(data = plot_data, aes(x = Rounds, y = Accuracy), color = "blue") +  # Plot the fitted curve
  labs(x = "Rounds", y = "Accuracy") +  # Set axis labels
  theme_minimal() +  # Set the plot theme
  ylim(0.72, 0.745)  # Set the y-axis limits

# Add a label for the maximum value
p <- p + annotate("text", x = max_round, y = max_accuracy, label = paste("Max Accuracy =", round(max_accuracy, 4)), vjust = -1, hjust = 0.5, color = "red")

# Add a label for the maximum value's corresponding round
p <- p + annotate("text", x = max_round, y = max_accuracy, label = paste("at Rounds =", round(max_round, 0)), vjust = 1, hjust = 0.5, color = "red")

# Display the plot
print(p)
```
The maximum accuracy is in the rounds interval 80-100, which is approximately 0.7371.
The rounds parameter in XGBoost determines the complexity of the model. A lower value of rounds can lead to underfitting, where the model is too simple to capture the underlying patterns in the data. Conversely, a higher value of rounds can result in overfitting, where the model becomes excessively complex and starts fitting the noise in the training data.

In this model, the rounds value is set to 100 to ensure that the model is accurate enough.

## Analysis of xgboost

The True Positive Rate provided by xgboost is about 0.738, which is almost equal to the max accuracy it provides.
Compared with the decision tree model, xgboost did not significantly improve the accuracy of prediction, which may be due to many reasons. One of the possible reasons is the data itself.Due to its characteristics, model regression in most cases may not provide higher accuracy unless more detailed or targeted processing is performed.

However, compared to accuracy, the most powerful thing about the xgboost model is its saving of computing time! After the number of rounds reaches more than 10 times, the change in accuracy has been less than 1%. In actual use, 10-20 boost rounds can meet the demand. Even when processing larger data, the xgboost algorithm will greatly reduce the calculation time. In other words, the xgboost algorithm is excellent in both efficiency and accuracy.

## Conclusion





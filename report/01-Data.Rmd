---
title: "Assessment 1 - 01: Data and Introduction"
output: html_notebook
---

## The brief

The brief was to:

* Consider a binary classification problem in which each group member will create a model submission that can be evaluated on left-out test data
* Together, agree and test a performance metric
* Compare our models according to that performance metric

## The team

We are a four-person team consisting of, *Jiaqi*, *Revati*, *Yifan*, and *Hugh*. We decided to split into pairs, Hugh and Yifan, Jiaqi and Revati, to contribute to R and Python analyses, respectively. We all intend to contribute equally to this project.

## Introduction and Problem Context

The data we selected is a large dataset with 22 body/health-related features, a ternary classification for smoking, and a binary classification for whether someone drinks or not.

Whilst the overall binary classification problem on the drinking target is clear, we find it important to provide a real-world scenario in which we will be working our analyses towards. 

We pose as analysts at a health insurance broker. The data has been collected from our policyholders, and we need to determine from this data whether they drink or not as to assign a higher premium if they do, given the far greater health risks associated with drinking.

* We acknowledge the incredibly unlikely situation where an insurance broker would have an insured person's gamma GTP level (one of the features) but not be able to ask whether they drink or not. However, we postulate our policyholders would be untruthful in answering this question, so we must predict the answer ourselves.

The main point of this is to frame our chosen performance metric, Sensitivity/Recall. From our contextual set up, we reason a false negative (assigning a lower premium to a drinker) could be rectified via reimbursement of the policy holder, while a false positive (failing to assign a higher premium to a drinker) would incur potential financial loss if we were to cover a health issue we hadn't collected sufficient funds to pay for.  Therefore, we determined the true positive rate to be the key performance indicator for our models.

## Library requirements

Each script handles its own requirements using a simple check-and-install system.

Separate requirements files are given for ease of use.

We need `fs` to be able to handle directories in a cross-platform way. This makes the whole analysis much more repeatable and is good data science practice.

```{r}
if (!require("fs")) install.packages("fs")
library("fs")
```

## The data

We found a good dataset for binary classification on Kaggle.

To obtain this dataset in a convenient format for both analyses, we will download it and process it into a standard form.

We place raw data in the `data` folder in the main branch.

* /data
  * /data/raw
    * /data/raw/smoking_driking_dataset.csv

### Get the data

First, create a directory for it to go in:
```{r}
rawdatadir=path_wd("..","data","raw")
if(!dir.exists(rawdatadir)) dir.create(rawdatadir,recursive = TRUE)
```

Download it
```{r}
download.file("https://raw.githubusercontent.com/EurousZhao/DST-group_A1/main/data/smoking_driking_dataset.csv",path_wd("..","data","raw","smoking_driking_dataset.csv"))
```

The data is already headed.

### Create the data output

```{r}
data <- read.table(path_wd("..","data","raw","smoking_driking_dataset.csv"),header=TRUE, sep = ",")
```

whilst we could've processed the data a little here, we decided to do that in the analyses to make a greater contribution to the narrative of understanding and processing the data.

### Summary

We have now created our data for further processing, in a simple headed tabular format.
